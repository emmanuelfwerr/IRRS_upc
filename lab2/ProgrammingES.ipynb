{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Modifying ElasticSearch index behavior\n",
    "\n",
    "In the previous session we had to clean manually the list of words in order to compute Zipf's and Heaps' laws. \n",
    "\n",
    "ElasticSearch allows using a pipeline of processes that allows to clean the text that is indexed discarding anything not useful.\n",
    "\n",
    "We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "\n",
    "ELASTIC_PASSWORD = \"lq2atUa_FxP=TI=hBPzb\"\n",
    "\n",
    "client = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=\"./credentials/http_ca.crt\",\n",
    "    http_auth=(\"elastic\", ELASTIC_PASSWORD),\n",
    "    timeout=1000,\n",
    ")\n",
    "\n",
    "index = Index('news', using=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Whitespace filter lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "ConnectionError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))) caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/elasticsearch/connection/http_urllib3.py:251\u001b[0m, in \u001b[0;36mUrllib3HttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    249\u001b[0m     request_headers[\u001b[39m\"\u001b[39m\u001b[39mcontent-encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 251\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    252\u001b[0m     method, url, body, retries\u001b[39m=\u001b[39;49mRetry(\u001b[39mFalse\u001b[39;49;00m), headers\u001b[39m=\u001b[39;49mrequest_headers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/util/retry.py:525\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m error:\n\u001b[1;32m    524\u001b[0m     \u001b[39m# Disabled, indicate to re-raise the error.\u001b[39;00m\n\u001b[0;32m--> 525\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    527\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/packages/six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    770\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m my_analyzer \u001b[38;5;241m=\u001b[39m analyzer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhitespace\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m ind \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m'\u001b[39m, using\u001b[38;5;241m=\u001b[39mclient)\n\u001b[0;32m---> 13\u001b[0m ind\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     14\u001b[0m ind\u001b[38;5;241m.\u001b[39manalyzer(my_analyzer)    \n\u001b[1;32m     15\u001b[0m ind\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/elasticsearch_dsl/index.py:396\u001b[0m, in \u001b[0;36mIndex.close\u001b[0;34m(self, using, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclose\u001b[39m(\u001b[39mself\u001b[39m, using\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    390\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39m    Closes the index in elasticsearch.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m    Any additional keyword arguments will be passed to\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39m    ``Elasticsearch.indices.close`` unchanged.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection(using)\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mclose(index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/elasticsearch/client/utils.py:168\u001b[0m, in \u001b[0;36mquery_params.<locals>._wrapper.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m p \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    167\u001b[0m         params[p] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(p)\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/elasticsearch/client/indices.py:264\u001b[0m, in \u001b[0;36mIndicesClient.close\u001b[0;34m(self, index, params, headers)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39min\u001b[39;00m SKIP_IN_PATH:\n\u001b[1;32m    262\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEmpty value passed for a required argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransport\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    265\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, _make_path(index, \u001b[39m\"\u001b[39;49m\u001b[39m_close\u001b[39;49m\u001b[39m\"\u001b[39;49m), params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders\n\u001b[1;32m    266\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/elasticsearch/transport.py:413\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[39m# raise exception on last retry\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[39mif\u001b[39;00m attempt \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries:\n\u001b[0;32m--> 413\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    414\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/elasticsearch/transport.py:381\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    378\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_connection()\n\u001b[1;32m    380\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     status, headers_response, data \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    382\u001b[0m         method,\n\u001b[1;32m    383\u001b[0m         url,\n\u001b[1;32m    384\u001b[0m         params,\n\u001b[1;32m    385\u001b[0m         body,\n\u001b[1;32m    386\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    387\u001b[0m         ignore\u001b[39m=\u001b[39;49mignore,\n\u001b[1;32m    388\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    389\u001b[0m     )\n\u001b[1;32m    391\u001b[0m \u001b[39mexcept\u001b[39;00m TransportError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/irrs_env/lib/python3.10/site-packages/elasticsearch/connection/http_urllib3.py:264\u001b[0m, in \u001b[0;36mUrllib3HttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m    263\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionTimeout(\u001b[39m\"\u001b[39m\u001b[39mTIMEOUT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(e), e)\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mN/A\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(e), e)\n\u001b[1;32m    266\u001b[0m \u001b[39m# raise warnings if any from the 'Warnings' header.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m warning_headers \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget_all(\u001b[39m\"\u001b[39m\u001b[39mwarning\u001b[39m\u001b[39m\"\u001b[39m, ())\n",
      "\u001b[0;31mConnectionError\u001b[0m: ConnectionError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))) caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))"
     ]
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('whitespace'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 13, u'token': u'4\\xeds', u'type': u'word', u'start_offset': 10, u'position': 2}\n",
      "{u'end_offset': 18, u'token': u'was%', u'type': u'word', u'start_offset': 14, u'position': 3}\n",
      "{u'end_offset': 28, u'token': u'&printing', u'type': u'word', u'start_offset': 19, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n",
      "{u'end_offset': 46, u'token': u'the.', u'type': u'word', u'start_offset': 42, u'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'<ALPHANUM>', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'<ALPHANUM>', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 13, u'token': u'4\\xeds', u'type': u'<ALPHANUM>', u'start_offset': 10, u'position': 2}\n",
      "{u'end_offset': 17, u'token': u'was', u'type': u'<ALPHANUM>', u'start_offset': 14, u'position': 3}\n",
      "{u'end_offset': 28, u'token': u'printing', u'type': u'<ALPHANUM>', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'<ALPHANUM>', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'<ALPHANUM>', u'start_offset': 37, u'position': 6}\n",
      "{u'end_offset': 45, u'token': u'the', u'type': u'<ALPHANUM>', u'start_offset': 42, u'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 13, u'token': u'\\xeds', u'type': u'word', u'start_offset': 11, u'position': 2}\n",
      "{u'end_offset': 17, u'token': u'was', u'type': u'word', u'start_offset': 14, u'position': 3}\n",
      "{u'end_offset': 28, u'token': u'printing', u'type': u'word', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n",
      "{u'end_offset': 45, u'token': u'the', u'type': u'word', u'start_offset': 42, u'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter asciifolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 13, u'token': u'is', u'type': u'word', u'start_offset': 11, u'position': 2}\n",
      "{u'end_offset': 17, u'token': u'was', u'type': u'word', u'start_offset': 14, u'position': 3}\n",
      "{u'end_offset': 28, u'token': u'printing', u'type': u'word', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n",
      "{u'end_offset': 45, u'token': u'the', u'type': u'word', u'start_offset': 42, u'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter asciifolding + stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding', 'stop']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 28, u'token': u'printing', u'type': u'word', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter asciifolding + stop + snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding','stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 28, u'token': u'print', u'type': u'word', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'print', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **follow the instructions** of the documentation, index the documents from the previous session using the script 'IndexFilesPreprocess.py' and use the script 'CountWords.py' from the previous session to see how the set of tokens change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2 The index reloaded\n",
    "\n",
    "You can use the modified indexer ```IndexFilesPreprocess.py``` script to play with the different possibilities for the preprocessing pipeline.\n",
    "\n",
    "You can change the **tokenizer** and apply different processes to the tokens like lowercasing, asccii folding, removing stopwords and different stemming algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3 Computing Tf-Idf and Cosine similarity\n",
    "\n",
    "Now is your turn to work in the session task.\n",
    "\n",
    "The idea is to program a script that given two document paths obtains their ids, computes the Tf-Idf representation of the documents and then computes and prints their cosine similarity\n",
    "\n",
    "**Follow the instructions** in the documentation and and **pay attention** to the documentation that you have to deliver for this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('irrs_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "853a85609c504845e321944cec600a636e76eb281e1962f3c7d40f878047530c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
